2026-02-03 10:23:07.0413 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 10:23:07.0413 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {},
    "embedding_models": {},
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "file",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null,
        "file_type": "text",
        "base_dir": "input"
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "file",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        },
        "base_dir": "cache"
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph_final.txt",
        "entity_types": [
            "PERSON",
            "ORGANIZATION",
            "GEO",
            "EVENT"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": null,
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": null,
        "text_prompt": null,
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": null,
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 10:23:07.0413 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 10:23:07.0413 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 10:30:26.0138 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 10:30:26.0140 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {},
    "embedding_models": {},
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "file",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null,
        "file_type": "text",
        "base_dir": "input"
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null,
            "azure_connection_string": null,
            "azure_container_name": null,
            "azure_account_url": null,
            "azure_cosmosdb_account_url": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph_final.txt",
        "entity_types": [
            "PERSON",
            "ORGANIZATION",
            "GEO",
            "EVENT"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": null,
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": null,
        "text_prompt": null,
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": null,
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 10:30:26.0142 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 10:30:26.0142 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 10:30:26.0152 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 10:30:26.0160 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 10:30:26.0160 - ERROR - graphrag.index.run.run_pipeline - error running workflow load_input_documents
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\load_input_documents.py", line 24, in run_workflow
    input_reader = create_input_reader(config.input, context.input_storage)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_input\input_reader_factory.py", line 86, in create_input_reader
    raise ValueError(msg)
ValueError: InputConfig.type 'file' is not registered in the InputReaderFactory. Registered types: .
2026-02-03 10:30:26.0163 - ERROR - graphrag.api.index - Workflow load_input_documents completed with errors
2026-02-03 10:33:39.0049 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 10:33:39.0049 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {},
    "embedding_models": {},
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "directory",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null,
        "base_dir": "input"
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null,
            "azure_connection_string": null,
            "azure_container_name": null,
            "azure_account_url": null,
            "azure_cosmosdb_account_url": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph_final.txt",
        "entity_types": [
            "PERSON",
            "ORGANIZATION",
            "GEO",
            "EVENT"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": null,
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": null,
        "text_prompt": null,
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": null,
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 10:33:39.0049 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 10:33:39.0049 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 10:33:39.0069 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 10:33:39.0076 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 10:33:39.0076 - ERROR - graphrag.index.run.run_pipeline - error running workflow load_input_documents
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\load_input_documents.py", line 24, in run_workflow
    input_reader = create_input_reader(config.input, context.input_storage)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_input\input_reader_factory.py", line 86, in create_input_reader
    raise ValueError(msg)
ValueError: InputConfig.type 'directory' is not registered in the InputReaderFactory. Registered types: .
2026-02-03 10:33:39.0078 - ERROR - graphrag.api.index - Workflow load_input_documents completed with errors
2026-02-03 10:36:33.0596 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 10:36:33.0596 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {},
    "embedding_models": {},
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": "*.txt",
        "id_column": null,
        "title_column": null,
        "text_column": null,
        "source": "./input"
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null,
            "azure_connection_string": null,
            "azure_container_name": null,
            "azure_account_url": null,
            "azure_cosmosdb_account_url": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph_final.txt",
        "entity_types": [
            "PERSON",
            "ORGANIZATION",
            "GEO",
            "EVENT"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": null,
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": null,
        "text_prompt": null,
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": null,
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 10:36:33.0609 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 10:36:33.0609 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 10:36:33.0615 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 10:36:33.0624 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 10:36:33.0642 - ERROR - graphrag.index.run.run_pipeline - error running workflow load_input_documents
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\load_input_documents.py", line 26, in run_workflow
    output = await load_input_documents(input_reader)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\load_input_documents.py", line 43, in load_input_documents
    return pd.DataFrame(await input_reader.read_files())
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_input\input_reader.py", line 37, in read_files
    files = list(self._storage.find(re.compile(self._file_pattern)))
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\re\__init__.py", line 228, in compile
    return _compile(pattern, flags)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\re\__init__.py", line 307, in _compile
    p = _compiler.compile(pattern, flags)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\re\_compiler.py", line 745, in compile
    p = _parser.parse(p, flags)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\re\_parser.py", line 979, in parse
    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\re\_parser.py", line 460, in _parse_sub
    itemsappend(_parse(source, state, verbose, nested + 1,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\re\_parser.py", line 687, in _parse
    raise source.error("nothing to repeat",
re.error: nothing to repeat at position 0
2026-02-03 10:36:33.0730 - ERROR - graphrag.api.index - Workflow load_input_documents completed with errors
2026-02-03 10:44:13.0633 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 10:44:13.0633 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {},
    "embedding_models": {},
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": ".*\\.txt",
        "id_column": null,
        "title_column": null,
        "text_column": null,
        "source": "./input"
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null,
            "azure_connection_string": null,
            "azure_container_name": null,
            "azure_account_url": null,
            "azure_cosmosdb_account_url": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph_final.txt",
        "entity_types": [
            "PERSON",
            "ORGANIZATION",
            "GEO",
            "EVENT"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": null,
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": null,
        "text_prompt": null,
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": null,
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 10:44:13.0633 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 10:44:13.0633 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 10:44:13.0649 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 10:44:13.0669 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 10:44:13.0694 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 10:44:14.0313 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 10:44:14.0338 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 10:44:14.0341 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 10:44:23.0952 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 10:44:23.0968 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 10:44:24.0003 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 10:44:24.0003 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 10:44:24.0013 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 10:44:24.0013 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 10:44:24.0034 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 10:44:24.0107 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 10:44:24.0107 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 10:44:24.0123 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 10:44:24.0123 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 10:44:24.0134 - ERROR - graphrag.index.run.run_pipeline - error running workflow extract_graph
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\extract_graph.py", line 40, in run_workflow
    extraction_model_config = config.get_completion_model_config(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\config\models\graph_rag_config.py", line 289, in get_completion_model_config
    raise ValueError(err_msg)
ValueError: Model ID default_completion_model not found in completion_models. Please rerun `graphrag init` and set the completion_models configuration.
2026-02-03 10:44:24.0136 - ERROR - graphrag.api.index - Workflow extract_graph completed with errors
2026-02-03 10:54:00.0606 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 10:54:00.0606 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {},
    "embedding_models": {},
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": ".*\\.txt",
        "id_column": null,
        "title_column": null,
        "text_column": null,
        "source": "./input"
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null,
            "azure_connection_string": null,
            "azure_container_name": null,
            "azure_account_url": null,
            "azure_cosmosdb_account_url": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "overwrite": true
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph_final.txt",
        "entity_types": [
            "PERSON",
            "ORGANIZATION",
            "GEO",
            "EVENT"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": null,
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": null,
        "text_prompt": null,
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": null,
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": null,
        "reduce_prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": null,
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 10:54:00.0606 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 10:54:00.0606 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 10:54:00.0622 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 10:54:00.0627 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 10:54:00.0640 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 10:54:00.0682 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 10:54:00.0696 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 10:54:00.0699 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 10:54:01.0562 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 10:54:01.0567 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 10:54:01.0594 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 10:54:01.0594 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 10:54:01.0610 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 10:54:01.0610 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 10:54:01.0626 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 10:54:01.0675 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 10:54:01.0675 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 10:54:01.0699 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 10:54:01.0701 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 10:54:01.0710 - ERROR - graphrag.index.run.run_pipeline - error running workflow extract_graph
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\extract_graph.py", line 40, in run_workflow
    extraction_model_config = config.get_completion_model_config(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\config\models\graph_rag_config.py", line 289, in get_completion_model_config
    raise ValueError(err_msg)
ValueError: Model ID default_completion_model not found in completion_models. Please rerun `graphrag init` and set the completion_models configuration.
2026-02-03 10:54:01.0713 - ERROR - graphrag.api.index - Workflow extract_graph completed with errors
2026-02-03 11:05:11.0779 - ERROR - graphrag_llm.middleware.with_logging - Request failed with exception=litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: <API_KEY>. You can find your API key at https://platform.openai.com/account/api-keys.
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\openai\openai.py", line 762, in completion
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\openai\openai.py", line 690, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 237, in sync_wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\openai\openai.py", line 502, in make_sync_openai_chat_completion_request
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\openai\openai.py", line 477, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\openai\_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\openai\_utils\_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\openai\_base_client.py", line 1294, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\openai\_base_client.py", line 1067, in request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: <API_KEY>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 2531, in completion
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 2503, in completion
    response = openai_chat_completions.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\openai\openai.py", line 773, in completion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: <API_KEY>. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 49, in _request_count_middleware
    return sync_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 53, in _request_count_middleware
    result = sync_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 47, in _retry_middleware
    return retrier.retry(
           ^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 70, in retry
    return func(**input_args)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 56, in _metrics_middleware
    response = sync_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 276, in _base_completion
    response = litellm.completion(
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1742, in wrapper
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1563, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4242, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 509, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: <API_KEY>. You can find your API key at https://platform.openai.com/account/api-keys.
2026-02-03 11:05:11.0794 - ERROR - graphrag.index.validate_config - LLM configuration error detected.
litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: <API_KEY>. You can find your API key at https://platform.openai.com/account/api-keys.
2026-02-03 11:05:11.0794 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for openai/default_chat_model: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 11:40:06.0337 - ERROR - graphrag_llm.middleware.with_logging - Request failed with exception=litellm.AuthenticationError: GeminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2755, in completion
    response = client.post(url=url, headers=headers, json=data, logging_obj=logging_obj)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 1010, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 992, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://generativelanguage.googleapis.com/v1beta/models/default_chat_model:generateContent?key=%3CAPI_KEY%3E'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 3275, in completion
    response = vertex_chat_completion.completion(  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2759, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 49, in _request_count_middleware
    return sync_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 53, in _request_count_middleware
    result = sync_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 47, in _retry_middleware
    return retrier.retry(
           ^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 70, in retry
    return func(**input_args)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 56, in _metrics_middleware
    response = sync_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 276, in _base_completion
    response = litellm.completion(
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1742, in wrapper
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1563, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4242, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1306, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: GeminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2026-02-03 11:40:06.0368 - ERROR - graphrag.index.validate_config - LLM configuration error detected.
litellm.AuthenticationError: GeminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2026-02-03 11:40:06.0368 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/default_chat_model: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 11:42:15.0000 - ERROR - graphrag_llm.middleware.with_logging - Request failed with exception=litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/default_chat_model is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2755, in completion
    response = client.post(url=url, headers=headers, json=data, logging_obj=logging_obj)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 1010, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 992, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://generativelanguage.googleapis.com/v1beta/models/default_chat_model:generateContent?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 3275, in completion
    response = vertex_chat_completion.completion(  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2759, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 404,
    "message": "models/default_chat_model is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 49, in _request_count_middleware
    return sync_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 53, in _request_count_middleware
    result = sync_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 47, in _retry_middleware
    return retrier.retry(
           ^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 70, in retry
    return func(**input_args)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 56, in _metrics_middleware
    response = sync_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 276, in _base_completion
    response = litellm.completion(
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1742, in wrapper
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1563, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4242, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1418, in exception_type
    raise NotFoundError(
litellm.exceptions.NotFoundError: litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/default_chat_model is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 11:42:15.0020 - ERROR - graphrag.index.validate_config - LLM configuration error detected.
litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/default_chat_model is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 11:42:15.0020 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/default_chat_model: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 11:43:50.0260 - ERROR - graphrag_llm.middleware.with_logging - Request failed with exception=litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2755, in completion
    response = client.post(url=url, headers=headers, json=data, logging_obj=logging_obj)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 1010, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 992, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 3275, in completion
    response = vertex_chat_completion.completion(  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2759, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 404,
    "message": "models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 49, in _request_count_middleware
    return sync_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 53, in _request_count_middleware
    result = sync_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 47, in _retry_middleware
    return retrier.retry(
           ^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 70, in retry
    return func(**input_args)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 56, in _metrics_middleware
    response = sync_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 276, in _base_completion
    response = litellm.completion(
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1742, in wrapper
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1563, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4242, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1418, in exception_type
    raise NotFoundError(
litellm.exceptions.NotFoundError: litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 11:43:50.0278 - ERROR - graphrag.index.validate_config - LLM configuration error detected.
litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 11:43:50.0279 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-1.5-flash: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 11:46:01.0183 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 11:46:02.0042 - ERROR - graphrag_llm.middleware.with_logging - Async request failed with exception=litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-3-flash-preview is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4419, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini_embeddings\batch_embed_content_handler.py", line 174, in async_batch_embeddings
    response = await async_handler.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:batchEmbedContents?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 55, in _retry_middleware_async
    return await retrier.retry_async(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 102, in retry_async
    return await func(**input_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\embedding\lite_llm_embedding.py", line 195, in _base_embedding_async
    response = await litellm.aembedding(**new_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4434, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1418, in exception_type
    raise NotFoundError(
litellm.exceptions.NotFoundError: litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-3-flash-preview is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 11:46:02.0054 - ERROR - graphrag.index.validate_config - Embedding configuration error detected.
litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-3-flash-preview is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 11:46:02.0054 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-3-flash-preview: {
  "attempted_request_count": 2,
  "successful_response_count": 1,
  "failed_response_count": 1,
  "failure_rate": 0.5,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 2.3881003856658936,
  "compute_duration_per_response_seconds": 2.3881003856658936,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 1,
  "prompt_tokens": 12,
  "completion_tokens": 2,
  "total_tokens": 14,
  "tokens_per_response": 14.0,
  "responses_with_cost": 1,
  "input_cost": 6e-06,
  "output_cost": 6e-06,
  "total_cost": 1.2e-05,
  "cost_per_response": 1.2e-05
}
2026-02-03 12:02:15.0961 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 12:02:17.0246 - ERROR - graphrag_llm.middleware.with_logging - Async request failed with exception=litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-2.5-flash is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4419, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini_embeddings\batch_embed_content_handler.py", line 174, in async_batch_embeddings
    response = await async_handler.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:batchEmbedContents?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 55, in _retry_middleware_async
    return await retrier.retry_async(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 102, in retry_async
    return await func(**input_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\embedding\lite_llm_embedding.py", line 195, in _base_embedding_async
    response = await litellm.aembedding(**new_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4434, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1418, in exception_type
    raise NotFoundError(
litellm.exceptions.NotFoundError: litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-2.5-flash is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 12:02:17.0254 - ERROR - graphrag.index.validate_config - Embedding configuration error detected.
litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/gemini-2.5-flash is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 12:02:17.0254 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 2,
  "successful_response_count": 1,
  "failed_response_count": 1,
  "failure_rate": 0.5,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 2.4793648719787598,
  "compute_duration_per_response_seconds": 2.4793648719787598,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 1,
  "prompt_tokens": 12,
  "completion_tokens": 23,
  "total_tokens": 35,
  "tokens_per_response": 35.0,
  "responses_with_cost": 1,
  "input_cost": 3.6e-06,
  "output_cost": 5.75e-05,
  "total_cost": 6.110000000000001e-05,
  "cost_per_response": 6.110000000000001e-05
}
2026-02-03 12:10:25.0441 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 12:10:26.0198 - ERROR - graphrag_llm.middleware.with_logging - Async request failed with exception=litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/text-embedding-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4419, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini_embeddings\batch_embed_content_handler.py", line 174, in async_batch_embeddings
    response = await async_handler.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://generativelanguage.googleapis.com/v1beta/models/text-embedding-001:batchEmbedContents?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 55, in _retry_middleware_async
    return await retrier.retry_async(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 102, in retry_async
    return await func(**input_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\embedding\lite_llm_embedding.py", line 195, in _base_embedding_async
    response = await litellm.aembedding(**new_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4434, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1418, in exception_type
    raise NotFoundError(
litellm.exceptions.NotFoundError: litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/text-embedding-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 12:10:26.0215 - ERROR - graphrag.index.validate_config - Embedding configuration error detected.
litellm.NotFoundError: GeminiException - {
  "error": {
    "code": 404,
    "message": "models/text-embedding-001 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
    "status": "NOT_FOUND"
  }
}

2026-02-03 12:10:26.0216 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-001: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 12:10:26.0216 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 1,
  "successful_response_count": 1,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 5.152438402175903,
  "compute_duration_per_response_seconds": 5.152438402175903,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 1,
  "prompt_tokens": 12,
  "completion_tokens": 27,
  "total_tokens": 39,
  "tokens_per_response": 39.0,
  "responses_with_cost": 1,
  "input_cost": 3.6e-06,
  "output_cost": 6.75e-05,
  "total_cost": 7.110000000000001e-05,
  "cost_per_response": 7.110000000000001e-05
}
2026-02-03 12:12:34.0994 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 12:12:37.0877 - ERROR - graphrag_llm.middleware.with_logging - Async request failed with exception=litellm.AuthenticationError: HuggingfaceException - {"error":"Invalid username or password."}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4419, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\huggingface\embedding\handler.py", line 296, in aembedding
    response = await client.post(api_base, headers=headers, data=json.dumps(data))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '401 Unauthorized' for url 'https://router.huggingface.co/hf-inference/pipeline/sentence-similarity/sentence-transformers/all-MiniLM-L6-v2'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 55, in _retry_middleware_async
    return await retrier.retry_async(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 102, in retry_async
    return await func(**input_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\embedding\lite_llm_embedding.py", line 195, in _base_embedding_async
    response = await litellm.aembedding(**new_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4434, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1620, in exception_type
    raise AuthenticationError(
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: HuggingfaceException - {"error":"Invalid username or password."}
2026-02-03 12:12:37.0894 - ERROR - graphrag.index.validate_config - Embedding configuration error detected.
litellm.AuthenticationError: HuggingfaceException - {"error":"Invalid username or password."}
2026-02-03 12:12:37.0894 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for huggingface/sentence-transformers/all-MiniLM-L6-v2: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 12:12:37.0894 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 1,
  "successful_response_count": 1,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 7.470942497253418,
  "compute_duration_per_response_seconds": 7.470942497253418,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 1,
  "prompt_tokens": 12,
  "completion_tokens": 23,
  "total_tokens": 35,
  "tokens_per_response": 35.0,
  "responses_with_cost": 1,
  "input_cost": 3.6e-06,
  "output_cost": 5.75e-05,
  "total_cost": 6.110000000000001e-05,
  "cost_per_response": 6.110000000000001e-05
}
2026-02-03 12:17:28.0569 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 12:17:30.0848 - ERROR - graphrag_llm.middleware.with_logging - Async request failed with exception=litellm.APIError: HuggingfaceException - {"error":"This authentication method does not have sufficient permissions to call Inference Providers on behalf of user amalnath"}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4419, in aembedding
    response = await init_response  # type: ignore
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\huggingface\embedding\handler.py", line 296, in aembedding
    response = await client.post(api_base, headers=headers, data=json.dumps(data))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://router.huggingface.co/hf-inference/pipeline/sentence-similarity/sentence-transformers/all-MiniLM-L6-v2'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\embedding\lite_llm_embedding.py", line 195, in _base_embedding_async
    response = await litellm.aembedding(**new_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4434, in aembedding
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1659, in exception_type
    raise APIError(
litellm.exceptions.APIError: litellm.APIError: HuggingfaceException - {"error":"This authentication method does not have sufficient permissions to call Inference Providers on behalf of user amalnath"}
2026-02-03 12:17:30.0857 - ERROR - graphrag.index.validate_config - Embedding configuration error detected.
litellm.APIError: HuggingfaceException - {"error":"This authentication method does not have sufficient permissions to call Inference Providers on behalf of user amalnath"}
2026-02-03 12:17:30.0859 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for huggingface/sentence-transformers/all-MiniLM-L6-v2: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 12:17:30.0859 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 1,
  "successful_response_count": 1,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 3.4029746055603027,
  "compute_duration_per_response_seconds": 3.4029746055603027,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 1,
  "prompt_tokens": 12,
  "completion_tokens": 30,
  "total_tokens": 42,
  "tokens_per_response": 42.0,
  "responses_with_cost": 1,
  "input_cost": 3.6e-06,
  "output_cost": 7.500000000000001e-05,
  "total_cost": 7.860000000000001e-05,
  "cost_per_response": 7.860000000000001e-05
}
2026-02-03 15:12:49.0208 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 15:12:50.0075 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 15:12:50.0075 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 15:12:50.0079 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 15:12:50.0084 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 15:12:50.0084 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 15:12:50.0092 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 15:12:50.0102 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 15:12:50.0123 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 15:12:50.0274 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 15:12:50.0290 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 15:12:50.0296 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:12:51.0115 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 15:12:51.0123 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 15:12:51.0149 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 15:12:51.0149 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 15:12:51.0167 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 15:12:51.0170 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:12:51.0179 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:12:51.0222 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 15:12:51.0222 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 15:12:51.0235 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 15:12:51.0243 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:13:16.0905 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 15:13:16.0938 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/7
2026-02-03 15:13:16.0938 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/7
2026-02-03 15:13:16.0938 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/7
2026-02-03 15:13:16.0938 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/7
2026-02-03 15:13:16.0943 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/7
2026-02-03 15:13:16.0943 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/7
2026-02-03 15:13:16.0943 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/7
2026-02-03 15:13:16.0970 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 15:13:16.0970 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 15:13:16.0995 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 15:13:16.0997 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:13:17.0027 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:13:17.0090 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 15:13:17.0091 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 15:13:17.0108 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 15:13:17.0108 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 15:13:17.0108 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 15:13:17.0108 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 15:13:17.0113 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:13:17.0139 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:13:17.0239 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 15:13:17.0239 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 15:13:17.0263 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 15:13:17.0263 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:13:17.0274 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:13:17.0284 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:13:17.0328 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 15:13:17.0330 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 15:13:17.0342 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 15:13:17.0346 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:13:17.0355 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:13:17.0363 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 15:13:17.0452 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2026-02-03 15:13:25.0145 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2026-02-03 15:13:25.0163 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 15:13:25.0163 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 15:13:25.0186 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 15:13:25.0186 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 15:13:25.0186 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:13:25.0225 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:13:25.0234 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 15:13:25.0290 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 15:13:26.0389 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 15:13:26.0389 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 15:13:27.0308 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 15:13:27.0455 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 15:13:27.0530 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 15:13:27.0537 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.7508776187896729,
  "compute_duration_per_response_seconds": 0.8754388093948364,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 70,
  "total_tokens": 70,
  "tokens_per_response": 35.0,
  "cost_per_response": 0.0
}
2026-02-03 15:13:27.0537 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 4,
  "successful_response_count": 4,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 34.93709850311279,
  "compute_duration_per_response_seconds": 8.734274625778198,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 4,
  "prompt_tokens": 5944,
  "completion_tokens": 5570,
  "total_tokens": 11514,
  "tokens_per_response": 2878.5,
  "responses_with_cost": 4,
  "input_cost": 0.0017832,
  "output_cost": 0.013925000000000002,
  "total_cost": 0.015708200000000002,
  "cost_per_response": 0.0039270500000000005
}
2026-02-03 15:16:21.0906 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 15:16:22.0869 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 15:16:22.0869 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 15:16:22.0869 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 15:16:22.0869 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 15:16:22.0869 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 15:16:22.0884 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 15:16:22.0884 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 15:16:22.0905 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 15:16:22.0954 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 15:16:22.0968 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 15:16:22.0970 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:16:23.0669 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 15:16:23.0684 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 15:16:23.0708 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 15:16:23.0708 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 15:16:23.0721 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 15:16:23.0724 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:16:23.0735 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:16:23.0767 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 15:16:23.0768 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 15:16:23.0770 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 15:16:23.0781 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:16:23.0889 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 15:16:23.0921 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/7
2026-02-03 15:16:23.0921 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/7
2026-02-03 15:16:23.0921 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/7
2026-02-03 15:16:23.0921 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/7
2026-02-03 15:16:23.0928 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/7
2026-02-03 15:16:23.0929 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/7
2026-02-03 15:16:23.0929 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/7
2026-02-03 15:16:23.0952 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 15:16:23.0954 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 15:16:23.0973 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 15:16:23.0976 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:16:23.0987 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:16:24.0030 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 15:16:24.0031 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 15:16:24.0047 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 15:16:24.0047 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 15:16:24.0049 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 15:16:24.0049 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 15:16:24.0050 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:16:24.0080 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:16:24.0162 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 15:16:24.0162 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 15:16:24.0178 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 15:16:24.0180 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:16:24.0186 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:16:24.0199 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:16:24.0241 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 15:16:24.0241 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 15:16:24.0254 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 15:16:24.0257 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:16:24.0268 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:16:24.0278 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 15:16:24.0332 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2026-02-03 15:16:24.0426 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2026-02-03 15:16:24.0441 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 15:16:24.0442 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 15:16:24.0451 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 15:16:24.0451 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 15:16:24.0451 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:16:24.0492 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:16:24.0502 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 15:16:24.0535 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 15:16:24.0800 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 15:16:24.0800 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 15:16:24.0834 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 15:16:24.0855 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 15:16:24.0869 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 15:16:24.0872 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.8490114212036133,
  "compute_duration_per_response_seconds": 0.9245057106018066,
  "cached_responses": 1,
  "cache_hit_rate": 0.5,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 70,
  "total_tokens": 70,
  "tokens_per_response": 35.0,
  "cost_per_response": 0.0
}
2026-02-03 15:16:24.0872 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 4,
  "successful_response_count": 4,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 36.20016622543335,
  "compute_duration_per_response_seconds": 9.050041556358337,
  "cached_responses": 3,
  "cache_hit_rate": 0.75,
  "streaming_responses": 0,
  "responses_with_tokens": 4,
  "prompt_tokens": 5944,
  "completion_tokens": 5575,
  "total_tokens": 11519,
  "tokens_per_response": 2879.75,
  "responses_with_cost": 4,
  "input_cost": 0.0017832,
  "output_cost": 0.013937500000000002,
  "total_cost": 0.0157207,
  "cost_per_response": 0.003930175
}
2026-02-03 15:17:32.0040 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 15:17:33.0004 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 15:17:33.0004 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 15:17:33.0004 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 15:17:33.0012 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 15:17:33.0012 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 15:17:33.0021 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 15:17:33.0027 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 15:17:33.0042 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 15:17:33.0089 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 15:17:33.0101 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 15:17:33.0105 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:17:33.0886 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 15:17:33.0888 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 15:17:33.0919 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 15:17:33.0919 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 15:17:33.0922 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 15:17:33.0929 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:17:33.0941 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:17:34.0002 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 15:17:34.0002 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 15:17:34.0026 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 15:17:34.0034 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:17:59.0497 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 15:17:59.0520 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/7
2026-02-03 15:17:59.0520 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/7
2026-02-03 15:17:59.0520 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/7
2026-02-03 15:18:02.0657 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/7
2026-02-03 15:18:02.0657 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/7
2026-02-03 15:18:02.0657 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/7
2026-02-03 15:18:02.0657 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/7
2026-02-03 15:18:02.0684 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 15:18:02.0684 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 15:18:02.0715 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 15:18:02.0717 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:18:02.0746 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:18:02.0807 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 15:18:02.0807 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 15:18:02.0824 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 15:18:02.0824 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 15:18:02.0824 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 15:18:02.0824 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 15:18:02.0829 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:18:02.0858 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:18:02.0938 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 15:18:02.0938 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 15:18:02.0954 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 15:18:02.0957 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:18:02.0965 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:18:02.0974 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:18:03.0020 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 15:18:03.0020 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 15:18:03.0035 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 15:18:03.0035 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:18:03.0046 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:18:03.0056 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 15:18:03.0108 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2026-02-03 15:18:14.0263 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2026-02-03 15:18:14.0282 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 15:18:14.0282 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 15:18:14.0304 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 15:18:14.0304 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 15:18:14.0306 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:18:14.0350 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:18:14.0362 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 15:18:14.0397 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 15:18:14.0657 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 15:18:14.0658 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 15:18:15.0158 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 15:18:15.0174 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 15:18:15.0194 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 15:18:15.0200 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.432727575302124,
  "compute_duration_per_response_seconds": 0.716363787651062,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 86,
  "total_tokens": 86,
  "tokens_per_response": 43.0,
  "cost_per_response": 0.0
}
2026-02-03 15:18:15.0200 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 5,
  "successful_response_count": 5,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 41.74193286895752,
  "compute_duration_per_response_seconds": 8.348386573791505,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 5,
  "prompt_tokens": 6155,
  "completion_tokens": 7247,
  "total_tokens": 13402,
  "tokens_per_response": 2680.4,
  "responses_with_cost": 5,
  "input_cost": 0.0018464999999999998,
  "output_cost": 0.0181175,
  "total_cost": 0.019964000000000003,
  "cost_per_response": 0.0039928
}
2026-02-03 15:19:31.0094 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 15:19:31.0888 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 15:19:31.0888 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 15:19:31.0888 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": [],
            "vector_size": 768
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "overwrite": true
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 15:19:31.0897 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 15:19:31.0897 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 15:19:31.0903 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 15:19:31.0912 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 15:19:31.0921 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 15:19:31.0967 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 15:19:31.0972 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 15:19:31.0984 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:19:32.0677 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 15:19:32.0694 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 15:19:32.0694 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 15:19:32.0708 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 15:19:32.0709 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 15:19:32.0709 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:19:32.0728 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:19:32.0758 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 15:19:32.0758 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 15:19:32.0772 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 15:19:32.0773 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:19:32.0876 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 15:19:32.0890 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/7
2026-02-03 15:19:32.0890 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/7
2026-02-03 15:19:32.0890 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/7
2026-02-03 15:19:32.0936 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/7
2026-02-03 15:19:32.0936 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/7
2026-02-03 15:19:32.0936 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/7
2026-02-03 15:19:32.0936 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/7
2026-02-03 15:19:33.0081 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 15:19:33.0082 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 15:19:33.0119 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 15:19:33.0120 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:19:33.0131 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:19:33.0183 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 15:19:33.0183 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 15:19:33.0203 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 15:19:33.0203 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 15:19:33.0204 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 15:19:33.0204 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 15:19:33.0206 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:19:33.0251 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:19:33.0325 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 15:19:33.0325 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 15:19:33.0351 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 15:19:33.0353 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:19:33.0362 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:19:33.0373 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:19:33.0418 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 15:19:33.0418 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 15:19:33.0431 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 15:19:33.0432 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:19:33.0445 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:19:33.0455 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 15:19:33.0508 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2026-02-03 15:19:33.0596 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2026-02-03 15:19:33.0613 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 15:19:33.0613 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 15:19:33.0631 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 15:19:33.0631 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 15:19:33.0634 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:19:33.0665 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:19:33.0675 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 15:19:33.0716 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 15:19:34.0013 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 15:19:34.0015 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 15:19:34.0041 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 15:19:34.0067 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 15:19:34.0067 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 15:19:34.0082 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.2637639045715332,
  "compute_duration_per_response_seconds": 0.6318819522857666,
  "cached_responses": 1,
  "cache_hit_rate": 0.5,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 86,
  "total_tokens": 86,
  "tokens_per_response": 43.0,
  "cost_per_response": 0.0
}
2026-02-03 15:19:34.0082 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 5,
  "successful_response_count": 5,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 41.50166702270508,
  "compute_duration_per_response_seconds": 8.300333404541016,
  "cached_responses": 4,
  "cache_hit_rate": 0.8,
  "streaming_responses": 0,
  "responses_with_tokens": 5,
  "prompt_tokens": 6155,
  "completion_tokens": 7246,
  "total_tokens": 13401,
  "tokens_per_response": 2680.2,
  "responses_with_cost": 5,
  "input_cost": 0.0018464999999999998,
  "output_cost": 0.018115000000000003,
  "total_cost": 0.0199615,
  "cost_per_response": 0.0039923
}
2026-02-03 15:20:14.0470 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 15:20:15.0456 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 15:20:15.0456 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 15:20:15.0458 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": [],
            "vector_size": 768
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "overwrite": true
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 15:20:15.0462 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 15:20:15.0462 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 15:20:15.0470 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 15:20:15.0479 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 15:20:15.0489 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 15:20:15.0538 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 15:20:15.0549 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 15:20:15.0551 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:20:16.0291 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 15:20:16.0293 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 15:20:16.0308 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 15:20:16.0308 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 15:20:16.0317 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 15:20:16.0319 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:20:16.0329 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:20:16.0359 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 15:20:16.0359 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 15:20:16.0372 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 15:20:16.0372 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:20:45.0687 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 15:20:45.0698 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/9
2026-02-03 15:20:45.0698 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/9
2026-02-03 15:20:45.0698 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/9
2026-02-03 15:20:45.0698 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/9
2026-02-03 15:20:45.0698 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/9
2026-02-03 15:20:45.0698 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/9
2026-02-03 15:20:45.0710 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/9
2026-02-03 15:20:45.0710 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/9
2026-02-03 15:20:45.0710 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/9
2026-02-03 15:20:45.0729 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 15:20:45.0730 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 15:20:45.0752 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 15:20:45.0754 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:20:45.0785 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:20:45.0841 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 15:20:45.0842 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 15:20:45.0859 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 15:20:45.0861 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 15:20:45.0861 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 15:20:45.0861 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 15:20:45.0863 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:20:45.0894 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:20:45.0971 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 15:20:45.0971 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 15:20:45.0989 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 15:20:45.0990 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:20:46.0001 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:20:46.0011 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:20:46.0052 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 15:20:46.0052 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 15:20:46.0059 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 15:20:46.0066 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:20:46.0074 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:20:46.0085 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 15:20:46.0150 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2026-02-03 15:21:00.0841 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/2
2026-02-03 15:21:10.0683 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 2/2
2026-02-03 15:21:10.0701 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 15:21:10.0701 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 15:21:10.0721 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 15:21:10.0721 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 15:21:10.0723 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:21:10.0758 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:21:10.0768 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 15:21:10.0803 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 15:21:11.0068 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 15:21:11.0071 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 15:21:11.0458 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 15:21:11.0512 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 15:21:11.0518 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 15:21:11.0522 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.3661656379699707,
  "compute_duration_per_response_seconds": 0.6830828189849854,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 77,
  "total_tokens": 77,
  "tokens_per_response": 38.5,
  "cost_per_response": 0.0
}
2026-02-03 15:21:11.0522 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 5,
  "successful_response_count": 5,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 70.01871609687805,
  "compute_duration_per_response_seconds": 14.003743219375611,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 5,
  "prompt_tokens": 8037,
  "completion_tokens": 12533,
  "total_tokens": 20570,
  "tokens_per_response": 4114.0,
  "responses_with_cost": 5,
  "input_cost": 0.0024111,
  "output_cost": 0.0313325,
  "total_cost": 0.0337436,
  "cost_per_response": 0.00674872
}
2026-02-03 15:22:54.0716 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 15:22:55.0454 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 15:22:55.0454 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 15:22:55.0461 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": [],
            "vector_size": 768
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        },
        "overwrite": true,
        "vector_size": 768
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 15:22:55.0464 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 15:22:55.0464 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 15:22:55.0472 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 15:22:55.0481 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 15:22:55.0491 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 15:22:55.0537 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 15:22:55.0539 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 15:22:55.0539 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:22:56.0243 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 15:22:56.0255 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 15:22:56.0270 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 15:22:56.0271 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 15:22:56.0274 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 15:22:56.0274 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 15:22:56.0290 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:22:56.0321 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 15:22:56.0321 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 15:22:56.0334 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 15:22:56.0336 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:23:20.0635 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 15:23:20.0651 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/10
2026-02-03 15:23:20.0651 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/10
2026-02-03 15:23:20.0651 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/10
2026-02-03 15:23:20.0657 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/10
2026-02-03 15:23:22.0672 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/10
2026-02-03 15:23:22.0672 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/10
2026-02-03 15:23:22.0675 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/10
2026-02-03 15:23:22.0675 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/10
2026-02-03 15:23:22.0675 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/10
2026-02-03 15:23:22.0676 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 10/10
2026-02-03 15:23:22.0694 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 15:23:22.0694 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 15:23:22.0723 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 15:23:22.0724 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:23:22.0760 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:23:22.0819 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 15:23:22.0822 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 15:23:22.0845 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 15:23:22.0845 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 15:23:22.0845 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 15:23:22.0845 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 15:23:22.0846 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:23:22.0877 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:23:22.0960 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 15:23:22.0960 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 15:23:22.0976 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 15:23:22.0979 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:23:22.0989 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:23:22.0999 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:23:23.0044 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 15:23:23.0044 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 15:23:23.0061 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 15:23:23.0069 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 15:23:23.0081 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:23:23.0090 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 15:23:23.0143 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 5
2026-02-03 15:23:34.0651 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/2
2026-02-03 15:27:47.0489 - ERROR - graphrag_llm.middleware.with_logging - Async request failed after 7 retries with exception=litellm.RateLimitError: litellm.RateLimitError: geminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 5.022340027s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "5s"
      }
    ]
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2539, in async_completion
    response = await client.post(
               ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 611, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2548, in async_completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 5.022340027s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "5s"
      }
    ]
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_cache.py", line 145, in _cache_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 55, in _retry_middleware_async
    return await retrier.retry_async(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 102, in retry_async
    return await func(**input_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 302, in _base_completion_async
    response = await litellm.acompletion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 630, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1355, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: geminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 5.022340027s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "5s"
      }
    ]
  }
}

2026-02-03 15:27:47.0508 - ERROR - graphrag.index.operations.summarize_communities.community_reports_extractor - error generating community report
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2539, in async_completion
    response = await client.post(
               ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 611, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2548, in async_completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 5.022340027s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "5s"
      }
    ]
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\summarize_communities\community_reports_extractor.py", line 82, in __call__
    response = await self._model.completion_async(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 205, in completion_async
    response = await self._completion_async(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_cache.py", line 145, in _cache_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 55, in _retry_middleware_async
    return await retrier.retry_async(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 102, in retry_async
    return await func(**input_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 302, in _base_completion_async
    response = await litellm.acompletion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 630, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1355, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: geminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 5.022340027s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "5s"
      }
    ]
  }
}

2026-02-03 15:27:47.0573 - ERROR - graphrag.index.operations.summarize_communities.summarize_communities - Community Report Extraction Error
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2539, in async_completion
    response = await client.post(
               ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 495, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 451, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 611, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2548, in async_completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 5.022340027s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "5s"
      }
    ]
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\summarize_communities\community_reports_extractor.py", line 82, in __call__
    response = await self._model.completion_async(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 205, in completion_async
    response = await self._completion_async(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 64, in _request_count_middleware_async
    return await async_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 72, in _request_count_middleware_async
    result = await async_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_cache.py", line 145, in _cache_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 55, in _retry_middleware_async
    return await retrier.retry_async(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 102, in retry_async
    return await func(**input_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 80, in _metrics_middleware_async
    response = await async_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 302, in _base_completion_async
    response = await litellm.acompletion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 2034, in wrapper_async
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1850, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 630, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1355, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: geminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 5.022340027s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "5s"
      }
    ]
  }
}

2026-02-03 15:27:47.0583 - WARNING - graphrag.index.operations.summarize_communities.summarize_communities - No report found for community: 1
2026-02-03 15:27:47.0583 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 2/2
2026-02-03 15:27:47.0606 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 15:27:47.0607 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 15:27:47.0626 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 15:27:47.0626 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 15:27:47.0627 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 15:27:47.0660 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 15:27:47.0670 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 15:27:47.0712 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 15:27:47.0978 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 15:27:47.0978 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 15:27:48.0822 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 15:27:48.0864 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 15:27:48.0870 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 15:27:48.0874 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.552457571029663,
  "compute_duration_per_response_seconds": 0.7762287855148315,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 102,
  "total_tokens": 102,
  "tokens_per_response": 51.0,
  "cost_per_response": 0.0
}
2026-02-03 15:27:48.0874 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 6,
  "successful_response_count": 5,
  "failed_response_count": 1,
  "failure_rate": 0.16666666666666666,
  "requests_with_retries": 1,
  "retries": 7,
  "retry_rate": 0.5384615384615384,
  "compute_duration_seconds": 39.64592528343201,
  "compute_duration_per_response_seconds": 7.929185056686402,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 5,
  "prompt_tokens": 6120,
  "completion_tokens": 7117,
  "total_tokens": 13237,
  "tokens_per_response": 2647.4,
  "responses_with_cost": 5,
  "input_cost": 0.0018359999999999997,
  "output_cost": 0.017792500000000003,
  "total_cost": 0.019628500000000004,
  "cost_per_response": 0.003925700000000001
}
2026-02-03 15:42:25.0499 - ERROR - graphrag_llm.middleware.with_logging - Request failed after 7 retries with exception=litellm.RateLimitError: litellm.RateLimitError: geminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 27.061932039s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "27s"
      }
    ]
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2755, in completion
    response = client.post(url=url, headers=headers, json=data, logging_obj=logging_obj)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 1010, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 992, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyCNPUYUQw34pGtvgahc8wK9_NkuB-uLfZU'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 3275, in completion
    response = vertex_chat_completion.completion(  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2759, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 27.061932039s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "27s"
      }
    ]
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 49, in _request_count_middleware
    return sync_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 53, in _request_count_middleware
    result = sync_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 47, in _retry_middleware
    return retrier.retry(
           ^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 70, in retry
    return func(**input_args)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 56, in _metrics_middleware
    response = sync_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 276, in _base_completion
    response = litellm.completion(
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1742, in wrapper
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1563, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4242, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1355, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: geminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 27.061932039s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "27s"
      }
    ]
  }
}

2026-02-03 15:42:25.0527 - ERROR - graphrag.index.validate_config - LLM configuration error detected.
litellm.RateLimitError: litellm.RateLimitError: geminiException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 27.061932039s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-2.5-flash"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "27s"
      }
    ]
  }
}

2026-02-03 15:42:25.0534 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 1,
  "retries": 7,
  "retry_rate": 0.875,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 15:49:36.0514 - ERROR - graphrag_llm.middleware.with_logging - Request failed after 7 retries with exception=litellm.BadRequestError: GeminiException BadRequestError - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 16.082455403s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "model": "gemini-2.5-flash",
              "location": "global"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "16s"
      }
    ]
  }
}
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2755, in completion
    response = client.post(url=url, headers=headers, json=data, logging_obj=logging_obj)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 1010, in post
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 992, in post
    response.raise_for_status()
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=AIzaSyBVX9l5-NQg6aZ6dC-gcGMapf0joPrHvPQ'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 3275, in completion
    response = vertex_chat_completion.completion(  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 2759, in completion
    raise VertexAIError(
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 16.082455403s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "model": "gemini-2.5-flash",
              "location": "global"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "16s"
      }
    ]
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_logging.py", line 49, in _request_count_middleware
    return sync_middleware(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_request_count.py", line 53, in _request_count_middleware
    result = sync_middleware(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_retries.py", line 47, in _retry_middleware
    return retrier.retry(
           ^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\retry\exponential_retry.py", line 70, in retry
    return func(**input_args)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\middleware\with_metrics.py", line 56, in _metrics_middleware
    response = sync_middleware(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_llm\completion\lite_llm_completion.py", line 276, in _base_completion
    response = litellm.completion(
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1742, in wrapper
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\utils.py", line 1563, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\main.py", line 4242, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2378, in exception_type
    raise e
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1314, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: litellm.BadRequestError: GeminiException BadRequestError - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 16.082455403s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "model": "gemini-2.5-flash",
              "location": "global"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "16s"
      }
    ]
  }
}

2026-02-03 15:49:36.0537 - ERROR - graphrag.index.validate_config - LLM configuration error detected.
litellm.BadRequestError: GeminiException BadRequestError - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 16.082455403s.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "model": "gemini-2.5-flash",
              "location": "global"
            },
            "quotaValue": "20"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "16s"
      }
    ]
  }
}

2026-02-03 15:49:36.0540 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 1,
  "successful_response_count": 0,
  "failed_response_count": 1,
  "failure_rate": 1.0,
  "requests_with_retries": 1,
  "retries": 7,
  "retry_rate": 0.875,
  "compute_duration_per_response_seconds": 0.0,
  "cache_hit_rate": 0.0,
  "tokens_per_response": 0.0,
  "cost_per_response": 0.0
}
2026-02-03 16:25:51.0232 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 16:25:52.0505 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 16:25:52.0505 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 16:25:52.0511 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": [],
            "vector_size": 768
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 16:25:52.0512 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 16:25:52.0512 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 16:25:52.0527 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 16:25:52.0536 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 16:25:52.0556 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 16:25:52.0698 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 16:25:52.0721 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 16:25:52.0721 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 16:25:53.0664 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 16:25:53.0671 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 16:25:53.0705 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 16:25:53.0705 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 16:25:53.0719 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 16:25:53.0723 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 16:25:53.0732 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:25:53.0775 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 16:25:53.0775 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 16:25:53.0787 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 16:25:53.0787 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:26:21.0552 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/9
2026-02-03 16:26:21.0609 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/9
2026-02-03 16:26:21.0661 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 16:26:21.0661 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 16:26:21.0688 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 16:26:21.0694 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:26:21.0771 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:26:21.0848 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 16:26:21.0849 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 16:26:21.0875 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 16:26:21.0875 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 16:26:21.0877 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 16:26:21.0877 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 16:26:21.0879 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:26:21.0913 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:26:22.0009 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 16:26:22.0009 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 16:26:22.0036 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 16:26:22.0043 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:26:22.0053 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:26:22.0068 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:26:22.0113 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 16:26:22.0113 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 16:26:22.0128 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 16:26:22.0128 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:26:22.0144 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:26:22.0157 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 16:26:22.0212 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2026-02-03 16:26:33.0784 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2026-02-03 16:26:33.0844 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 16:26:33.0901 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 16:26:33.0934 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 16:26:33.0934 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 16:26:33.0934 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:26:34.0000 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:26:34.0011 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 16:26:34.0054 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 16:26:34.0745 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 16:26:34.0748 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 16:26:35.0480 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 16:26:35.0546 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 16:26:35.0562 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 16:26:35.0565 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.970045566558838,
  "compute_duration_per_response_seconds": 0.985022783279419,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 77,
  "total_tokens": 77,
  "tokens_per_response": 38.5,
  "cost_per_response": 0.0
}
2026-02-03 16:26:35.0566 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 4,
  "successful_response_count": 4,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 41.22153568267822,
  "compute_duration_per_response_seconds": 10.305383920669556,
  "cache_hit_rate": 0.0,
  "streaming_responses": 0,
  "responses_with_tokens": 4,
  "prompt_tokens": 5956,
  "completion_tokens": 7101,
  "total_tokens": 13057,
  "tokens_per_response": 3264.25,
  "responses_with_cost": 4,
  "input_cost": 0.0017867999999999998,
  "output_cost": 0.0177525,
  "total_cost": 0.019539300000000002,
  "cost_per_response": 0.004884825000000001
}
2026-02-03 16:35:25.0120 - INFO - graphrag.index.validate_config - LLM Config Params Validated
2026-02-03 16:35:25.0851 - INFO - graphrag.index.validate_config - Embedding LLM Config Params Validated
2026-02-03 16:35:25.0851 - INFO - graphrag.cli.index - Starting pipeline run. False
2026-02-03 16:35:25.0851 - INFO - graphrag.cli.index - Using default configuration: {
    "completion_models": {
        "default_completion_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "gemini-2.5-flash",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": []
        }
    },
    "embedding_models": {
        "default_embedding_model": {
            "type": "litellm",
            "model_provider": "gemini",
            "model": "text-embedding-004",
            "call_args": {},
            "api_base": null,
            "api_version": null,
            "api_key": "==== REDACTED ====",
            "auth_method": "api_key",
            "azure_deployment_name": null,
            "retry": {
                "type": "exponential_backoff",
                "max_retries": null,
                "base_delay": null,
                "jitter": null,
                "max_delay": null
            },
            "rate_limit": null,
            "metrics": {
                "type": "default",
                "store": "memory",
                "writer": "log",
                "log_level": null,
                "base_dir": null
            },
            "mock_responses": [],
            "vector_size": 768
        }
    },
    "concurrent_requests": 25,
    "async_mode": "threaded",
    "input": {
        "type": "text",
        "encoding": null,
        "file_pattern": null,
        "id_column": null,
        "title_column": null,
        "text_column": null
    },
    "input_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\input",
        "account_url": null,
        "database_name": null
    },
    "chunking": {
        "type": "tokens",
        "encoding_model": "o200k_base",
        "size": 1200,
        "overlap": 100,
        "prepend_metadata": null
    },
    "output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output",
        "account_url": null,
        "database_name": null
    },
    "update_output_storage": {
        "type": "file",
        "encoding": null,
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\update_output",
        "account_url": null,
        "database_name": null
    },
    "cache": {
        "type": "json",
        "storage": {
            "type": "file",
            "encoding": null,
            "base_dir": "cache",
            "account_url": null,
            "database_name": null
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\logs",
        "account_url": null
    },
    "vector_store": {
        "type": "lancedb",
        "db_uri": "C:\\Users\\amaln\\OneDrive\\Desktop\\GraphRag\\output\\lancedb",
        "url": null,
        "audience": null,
        "database_name": null,
        "index_schema": {
            "entity_description": {
                "index_name": "entity_description",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "community_full_content": {
                "index_name": "community_full_content",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            },
            "text_unit_text": {
                "index_name": "text_unit_text",
                "id_field": "id",
                "vector_field": "vector",
                "vector_size": 3072
            }
        }
    },
    "workflows": null,
    "embed_text": {
        "embedding_model_id": "default_embedding_model",
        "model_instance_name": "text_embedding",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "names": [
            "entity_description",
            "community_full_content",
            "text_unit_text"
        ]
    },
    "extract_graph": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_graph",
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1
    },
    "summarize_descriptions": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "summarize_descriptions",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "max_input_tokens": 4000
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": [
                "stuff",
                "thing",
                "things",
                "bunch",
                "bit",
                "bits",
                "people",
                "person",
                "okay",
                "hey",
                "hi",
                "hello",
                "laughter",
                "oh"
            ],
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25,
        "async_mode": "threaded"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40.0,
        "remove_ego_nodes": true,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "extract_claims": {
        "enabled": false,
        "completion_model_id": "default_completion_model",
        "model_instance_name": "extract_claims",
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1
    },
    "community_reports": {
        "completion_model_id": "default_completion_model",
        "model_instance_name": "community_reporting",
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_graph": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_context_tokens": 12000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "max_context_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_length": 1000,
        "reduce_max_length": 2000,
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "data_max_tokens": 12000,
        "reduce_max_tokens": null,
        "reduce_temperature": 0,
        "reduce_max_completion_tokens": null,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": null,
        "local_search_llm_max_gen_completion_tokens": null
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "completion_model_id": "default_completion_model",
        "embedding_model_id": "default_embedding_model",
        "k": 10,
        "max_context_tokens": 12000
    }
}
2026-02-03 16:35:25.0851 - INFO - graphrag.api.index - Initializing indexing pipeline...
2026-02-03 16:35:25.0851 - INFO - graphrag.index.workflows.factory - Creating pipeline with workflows: ['load_input_documents', 'create_base_text_units', 'create_final_documents', 'extract_graph', 'finalize_graph', 'extract_covariates', 'create_communities', 'create_final_text_units', 'create_community_reports', 'generate_text_embeddings']
2026-02-03 16:35:25.0866 - INFO - graphrag.index.run.run_pipeline - Running standard indexing.
2026-02-03 16:35:25.0866 - INFO - graphrag.index.run.run_pipeline - Executing pipeline...
2026-02-03 16:35:25.0883 - INFO - graphrag.index.workflows.load_input_documents - Final # of rows loaded: 1
2026-02-03 16:35:26.0017 - INFO - graphrag.api.index - Workflow load_input_documents completed successfully
2026-02-03 16:35:26.0037 - INFO - graphrag.index.workflows.create_base_text_units - Workflow started: create_base_text_units
2026-02-03 16:35:26.0037 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 16:35:26.0831 - INFO - graphrag.index.workflows.create_base_text_units - Starting chunking process for 1 documents
2026-02-03 16:35:26.0831 - INFO - graphrag.index.workflows.create_base_text_units - chunker progress:  1/1
2026-02-03 16:35:26.0860 - INFO - graphrag.index.workflows.create_base_text_units - Workflow completed: create_base_text_units
2026-02-03 16:35:26.0860 - INFO - graphrag.api.index - Workflow create_base_text_units completed successfully
2026-02-03 16:35:26.0869 - INFO - graphrag.index.workflows.create_final_documents - Workflow started: create_final_documents
2026-02-03 16:35:26.0869 - INFO - graphrag.utils.storage - reading table from storage: documents.parquet
2026-02-03 16:35:26.0882 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:35:26.0936 - INFO - graphrag.index.workflows.create_final_documents - Workflow completed: create_final_documents
2026-02-03 16:35:26.0936 - INFO - graphrag.api.index - Workflow create_final_documents completed successfully
2026-02-03 16:35:26.0944 - INFO - graphrag.index.workflows.extract_graph - Workflow started: extract_graph
2026-02-03 16:35:26.0952 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:35:27.0106 - INFO - graphrag.logger.progress - extract graph progress: 1/1
2026-02-03 16:35:27.0130 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 1/9
2026-02-03 16:35:27.0130 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 2/9
2026-02-03 16:35:27.0136 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 3/9
2026-02-03 16:35:27.0136 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 4/9
2026-02-03 16:35:27.0138 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 5/9
2026-02-03 16:35:27.0138 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 6/9
2026-02-03 16:35:27.0139 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 7/9
2026-02-03 16:35:27.0139 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 8/9
2026-02-03 16:35:27.0140 - INFO - graphrag.logger.progress - Summarize entity/relationship description progress: 9/9
2026-02-03 16:35:27.0161 - INFO - graphrag.index.workflows.extract_graph - Workflow completed: extract_graph
2026-02-03 16:35:27.0161 - INFO - graphrag.api.index - Workflow extract_graph completed successfully
2026-02-03 16:35:27.0184 - INFO - graphrag.index.workflows.finalize_graph - Workflow started: finalize_graph
2026-02-03 16:35:27.0186 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:35:27.0199 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:35:27.0250 - INFO - graphrag.index.workflows.finalize_graph - Workflow completed: finalize_graph
2026-02-03 16:35:27.0253 - INFO - graphrag.api.index - Workflow finalize_graph completed successfully
2026-02-03 16:35:27.0276 - INFO - graphrag.index.workflows.extract_covariates - Workflow started: extract_covariates
2026-02-03 16:35:27.0276 - INFO - graphrag.index.workflows.extract_covariates - Workflow completed: extract_covariates
2026-02-03 16:35:27.0276 - INFO - graphrag.api.index - Workflow extract_covariates completed successfully
2026-02-03 16:35:27.0276 - INFO - graphrag.index.workflows.create_communities - Workflow started: create_communities
2026-02-03 16:35:27.0281 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:35:27.0314 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:35:27.0405 - INFO - graphrag.index.workflows.create_communities - Workflow completed: create_communities
2026-02-03 16:35:27.0415 - INFO - graphrag.api.index - Workflow create_communities completed successfully
2026-02-03 16:35:27.0426 - INFO - graphrag.index.workflows.create_final_text_units - Workflow started: create_final_text_units
2026-02-03 16:35:27.0426 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:35:27.0441 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:35:27.0452 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:35:27.0498 - INFO - graphrag.index.workflows.create_final_text_units - Workflow completed: create_final_text_units
2026-02-03 16:35:27.0500 - INFO - graphrag.api.index - Workflow create_final_text_units completed successfully
2026-02-03 16:35:27.0507 - INFO - graphrag.index.workflows.create_community_reports - Workflow started: create_community_reports
2026-02-03 16:35:27.0507 - INFO - graphrag.utils.storage - reading table from storage: relationships.parquet
2026-02-03 16:35:27.0524 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:35:27.0534 - INFO - graphrag.utils.storage - reading table from storage: communities.parquet
2026-02-03 16:35:27.0593 - INFO - graphrag.index.operations.summarize_communities.graph_context.context_builder - Number of nodes at level=0 => 4
2026-02-03 16:35:27.0698 - INFO - graphrag.logger.progress - level 0 summarize communities progress: 1/1
2026-02-03 16:35:27.0709 - INFO - graphrag.index.workflows.create_community_reports - Workflow completed: create_community_reports
2026-02-03 16:35:27.0709 - INFO - graphrag.api.index - Workflow create_community_reports completed successfully
2026-02-03 16:35:27.0736 - INFO - graphrag.index.workflows.generate_text_embeddings - Workflow started: generate_text_embeddings
2026-02-03 16:35:27.0736 - INFO - graphrag.index.workflows.generate_text_embeddings - Embedding the following fields: ['entity_description', 'community_full_content', 'text_unit_text']
2026-02-03 16:35:27.0739 - INFO - graphrag.utils.storage - reading table from storage: text_units.parquet
2026-02-03 16:35:27.0771 - INFO - graphrag.utils.storage - reading table from storage: entities.parquet
2026-02-03 16:35:27.0781 - INFO - graphrag.utils.storage - reading table from storage: community_reports.parquet
2026-02-03 16:35:27.0823 - INFO - graphrag.index.workflows.generate_text_embeddings - Creating embeddings
2026-02-03 16:35:28.0434 - INFO - graphrag.index.operations.embed_text.embed_text - uploading text embeddings batch 1/1 of size 16 to vector store
2026-02-03 16:35:28.0434 - INFO - graphrag.index.operations.embed_text.run_embed_text - embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, batch_max_tokens=8191
2026-02-03 16:35:28.0464 - INFO - graphrag.logger.progress - generate embeddings progress: 1/1
2026-02-03 16:35:28.0520 - ERROR - graphrag.index.run.run_pipeline - error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 119, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 71, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 141, in generate_text_embeddings
    outputs[field] = await _run_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 173, in _run_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 86, in embed_text
    vector_store.load_documents(documents)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\graphrag_vectors\lancedb.py", line 90, in load_documents
    self.document_collection.add(data)
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 2242, in add
    return LOOP.run(
           ^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\background_loop.py", line 25, in run
    return asyncio.run_coroutine_threadsafe(future, self.loop).result()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 316, in __step_run_and_handle_result
    result = coro.throw(exc)
             ^^^^^^^^^^^^^^^
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 3406, in add
    return await self._inner.add(data, mode or "append")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\tasks.py", line 385, in __wakeup
    future.result()
  File "C:\Users\amaln\AppData\Local\Programs\Python\Python312\Lib\asyncio\futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)
RuntimeError: lance error: LanceError(Arrow): Arrow error: C Data interface error: Type error: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]. Detail: Python exception: Traceback (most recent call last):
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\lancedb\table.py", line 303, in gen
    yield pa.Table.from_batches([batch]).cast(reordered_schema).to_batches()[0]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4727, in pyarrow.lib.Table.cast
  File "pyarrow/table.pxi", line 597, in pyarrow.lib.ChunkedArray.cast
  File "C:\Users\amaln\OneDrive\Desktop\GraphRag\venv\Lib\site-packages\pyarrow\compute.py", line 412, in cast
    return call_function("cast", [arr], options, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_compute.pyx", line 604, in pyarrow._compute.call_function
  File "pyarrow/_compute.pyx", line 399, in pyarrow._compute.Function.call
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: Size of FixedSizeList is not the same. input list: fixed_size_list<item: float>[768] output list: fixed_size_list<item: float>[3072]
, C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\lance-datafusion-0.33.0\src\utils.rs:49:31
2026-02-03 16:35:28.0531 - ERROR - graphrag.api.index - Workflow generate_text_embeddings completed with errors
2026-02-03 16:35:28.0533 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/text-embedding-004: {
  "attempted_request_count": 2,
  "successful_response_count": 2,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 1.4331634044647217,
  "compute_duration_per_response_seconds": 0.7165817022323608,
  "cached_responses": 1,
  "cache_hit_rate": 0.5,
  "streaming_responses": 0,
  "responses_with_tokens": 2,
  "prompt_tokens": 77,
  "total_tokens": 77,
  "tokens_per_response": 38.5,
  "cost_per_response": 0.0
}
2026-02-03 16:35:28.0533 - INFO - graphrag_llm.metrics.log_metrics_writer - Metrics for gemini/gemini-2.5-flash: {
  "attempted_request_count": 4,
  "successful_response_count": 4,
  "failed_response_count": 0,
  "failure_rate": 0.0,
  "requests_with_retries": 0,
  "retries": 0,
  "retry_rate": 0.0,
  "compute_duration_seconds": 41.47700071334839,
  "compute_duration_per_response_seconds": 10.369250178337097,
  "cached_responses": 3,
  "cache_hit_rate": 0.75,
  "streaming_responses": 0,
  "responses_with_tokens": 4,
  "prompt_tokens": 5956,
  "completion_tokens": 7101,
  "total_tokens": 13057,
  "tokens_per_response": 3264.25,
  "responses_with_cost": 4,
  "input_cost": 0.0017867999999999998,
  "output_cost": 0.0177525,
  "total_cost": 0.019539300000000002,
  "cost_per_response": 0.004884825000000001
}
